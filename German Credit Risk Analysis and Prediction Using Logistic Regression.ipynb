{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "822a9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aad6de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'german_credit_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load your dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgerman_credit_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Drop duplicates\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'german_credit_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"german_credit_data.csv\")\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "# Identify missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "# Define categorical columns with missing values\n",
    "categorical_columns = ['Saving accounts', 'Checking account']\n",
    "# Fill missing values with mode for each categorical column\n",
    "for col in categorical_columns:\n",
    "    if df[col].isnull().any():\n",
    "        mode_value = df[col].mode()[0]  # Calculate mode\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "# Check for missing values after filling\n",
    "missing_vals = df.isnull().sum()\n",
    "print(\"Columns with missing values after filling:\")\n",
    "print(missing_vals[missing_vals > 0])\n",
    "# Calculate z-scores to identify outliers\n",
    "#z_scores = np.abs(stats.zscore(df_numeric))\n",
    "z_scores=np.abs(stats.zscore(df.select_dtypes(include=['float64', 'int64'])))\n",
    "# Remove outliers based on z-scores\n",
    "df = df[(z_scores < 3).all(axis=1)]\n",
    "# Identify inconsistent rows\n",
    "inco = df[df.apply(lambda x: x.astype(str).str.contains('inconsistent_value')).any(axis=1)]\n",
    "# Remove inconsistent rows\n",
    "df = df[~df.index.isin(inco.index)]\n",
    "\n",
    "#unique columns of categorical variables\n",
    "cat=df.select_dtypes(include=['object']).columns\n",
    "for column in cat:\n",
    "    unique=df[column].unique()\n",
    "    print(f\"unique values in column '{column}':{unique}\")\n",
    "#irrelevant column\n",
    "df=df.drop(df.columns[0], axis=1)\n",
    "# Save the cleaned dataframe\n",
    "df.to_csv(\"cleaned_german.csv\", index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7daf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"cleaned_german.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94625f6",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary statistics\n",
    "df.describe(include='all')\n",
    "# Saving accounts, Checking account,Housing, Purpose and Risk are categorical variables implying no mean,std,min,quartiles range etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014753a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histograms to visualize the distributions\n",
    "df.hist(figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "accda3a0",
   "metadata": {},
   "source": [
    "\n",
    "1. Age is right skewed suggesting that most individuals applying for the credit are young adults with fewer individuals being older.\n",
    "2.Job is not following normal distribution:indicating job types distribution is not symmetrical around the mean.This cld imply that theres a disproportionate representation of certain occupations in the dataset.\n",
    "3.Credit amount is right skewed:suggests small loan amounts are more prevalent in the dataset with fewer  instances of larger amounts.\n",
    "4 Distribution Duration is assymetrical:This indicates that certain loan terms are common than others among credit applicants\n",
    "this could reflect preferred loan durations among borrowers or lending policies that offer specific loan terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the correlation between Credit amount and Duration?\n",
    "dft=df.loc[:, ['Credit amount', 'Duration']]\n",
    "corr_matrix=dft.corr()\n",
    "print(corr_matrix)\n",
    "#heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3ed7b6c",
   "metadata": {},
   "source": [
    "- corr coefficient of 0.62 btwn credit amount and duration indicates moderately strong +ve relationship implying that the two vars are likely to influence each other in ameaningful way\n",
    "-This correlation suggests that higher credit amounts tend to be associated with longer durations &viceversa\n",
    "eg borrowers seeking larger credit amounts may opt for longer repayment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63326d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the relationship between savings and risk assessment in the credit scoring dataset, and why do applicants with substantial savings tend to have lower perceived risk?\n",
    "high_risk_high_savings = df[(df['Saving accounts'] == 'quite rich') | \n",
    "                            (df['Saving accounts'] == 'rich') & \n",
    "                            (df['Risk'] == 'high')].head(10)\n",
    "print(\"Applicants with substantial savings but high risk:\")\n",
    "print(high_risk_high_savings)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59d04a62",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "-Some people in the dataset have a lot of money saved.\n",
    "-Even though they have this money saved up, they are seen as safe borrowers.\n",
    "-This suggests that having savings might make lenders feel more comfortable about lending money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interesting case 2\n",
    "#Which applicants have a high credit amount but a short duration for repayment?\n",
    "high_credit_short_duration = df[(df['Credit amount'] > df['Credit amount'].quantile(0.75)) & \n",
    "                                (df['Duration'] < df['Duration'].quantile(0.25))].head(10)\n",
    "\n",
    "print(\"Applicants with high credit amounts but short duration for repayment:\")\n",
    "print(high_credit_short_duration)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fad75e6",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "People in the dataset are borrowing a lot of money but choosing to pay it back quickly. This might mean they want to avoid paying too much interest or they prefer getting rid of debt faster.it also suggests they might have limited financial flexibility, needing to pay off the loan quickly, possibly due to urgent needs or financial constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe980bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which applicants have a credit amount significantly higher than the average for their job type?\n",
    "average_credit_by_job=df.groupby('Job')['Credit amount'].mean()\n",
    "interesting_case_3=df[df['Credit amount']>average_credit_by_job[df['Job']].values+500].head(10)\n",
    "print(\"selected interesting cases for presentation:\")\n",
    "print(interesting_case_3[['Age', 'Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account', 'Credit amount', 'Duration', 'Purpose', 'Risk']])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "764ef29b",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "People from various backgrounds are borrowing larger amounts than usual for their jobs.\n",
    "This could signal significant purchases or investments.\n",
    "However, some are considered risky borrowers, suggesting potential repayment challenges."
   ]
  },
  {
   "cell_type": "raw",
   "id": "62025f61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1b294955",
   "metadata": {},
   "source": [
    "\n",
    "The most important variables for modelling could be:\n",
    "1.Credit amount:crucial for assessing financial need and repayment capacity \n",
    "2.Duration:loan term reflects the borrower's commitment and ability to repay the loan over specified period \n",
    "3.Job: indicates stability and income level, affecting repayment ability \n",
    "4.purpose:provides insights into loan intention, influencing risk assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a95b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q E\n",
    "df[\"Risk\"] = df[\"Risk\"].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd76402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Risk\"] = df[\"Risk\"].cat.codes\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"cleaned_german.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Risk', axis=1)  # Features\n",
    "y = data['Risk']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c77d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Sex', 'Housing', 'Saving accounts', 'Checking account', 'Purpose']\n",
    "numeric_features = ['Age', 'Job', 'Credit amount', 'Duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', LogisticRegression())])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432de8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbcd8567",
   "metadata": {},
   "source": [
    "confusion matrix is a table used to evaluate the perfomance of the model\n",
    "TP=10 instances where the model correctly predicted high risk\n",
    "FN=50 instances where the model predicted low risk but the actual risk was high.\n",
    "FP=6 instances where the model predicted high risk but the actual risk was low.\n",
    "TN=126 instances where the model correctly predicted low risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f35440ba",
   "metadata": {},
   "source": [
    "classification report is a perfomance evaluation tool for classification models\n",
    "accuracy of the model is 0.71:71% of the predictions made by the model are correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169e754",
   "metadata": {},
   "source": [
    "# Data Insights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd2de407",
   "metadata": {},
   "source": [
    "-Applicants with substantial savings tend to be perceived as lower risk, but exceptions exist.\n",
    "-High credit amounts are often associated with longer repayment durations.\n",
    "-Some applicants borrow large amounts relative to their job type, which could indicate significant purchases or financial strain.\n",
    "-The logistic regression model shows decent predictive power, with an accuracy of 71%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98d3f6",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c873b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
